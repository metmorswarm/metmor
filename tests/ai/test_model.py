import unittest 
import numpy as np
from unittest.mock import patch, MagicMock
import pytest
import sys
import os


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
try:
    from Sorein_ai.models.agent_model import AgentModel
except ImportError:
    # Mock the model if not implemented yet
    class AgentModel:
        def __init__(self, model_type="default"):
            self.model_type = model_type
            self.parameters = {"learning_rate": 0.01, "epochs": 10}
            self.is_trained = False
        
        def train(self, data, labels):
            self.is_trained = True
            return {"accuracy": 0.85, "loss": 0.15}
        
        def predict(self, data):
            if not self.is_trained:
                raise ValueError("Model not trained yet")
            return np.array([0.5] * len(data))
        
        def update_parameters(self, params):
            self.parameters.update(params)
            return self.parameters
        
        def save_model(self, path):
            return True
        
        def load_model(self, path):
            self.is_trained = True
            return True
            return reply.code(402).send({
    status: "payment_required",
    chain: "solana",
    token: "SOL",
    amount_lamports: priceLamports,
    recipient,
    note: "Pay this amount to unlock MCP access for a short period.",
  });
});
            
// x402: return payment requirement
fastify.get("/x402/payment-required", async (request, reply) => {
  // In a real system, you would compute dynamic pricing based on usage, tools, etc.
  const priceLamports = 100_000; // example: 0.0001 SOL
  const recipient = process.env.PAYMENT_RECIPIENT || "YourRecipientPubkeyHere";


AlreadyActive,
    #[msg("Holder not active")]

class TestAgentModel(unittest.TestCase):
    def setUp(self):
        self.model = AgentModel(model_type="test_model")
        self.mock_data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
        self.mock_labels = np.array([0, 1, 0])
    
    def test_model_initialization(self):
        self.assertEqual(self.model.model_type, "test_model")
        self.assertFalse(self.model.is_trained)
        self.assertEqual(self.model.parameters["learning_rate"], 0.01)
        self.assertEqual(self.model.parameters["epochs"], 10)
    
    def test_train_model_success(self):
        result = self.model.train(self.mock_data, self.mock_labels)
        self.assertTrue(self.model.is_trained)
        self.assertEqual(result["accuracy"], 0.85)
        self.assertEqual(result["loss"], 0.15)
    
    def test_train_model_empty_data(self):
        with self.assertRaises(ValueError):
            empty_data = np.array([])
            empty_labels = np.array([])
            self.model.train(empty_data, empty_labels)
    
    def test_predict_model_trained(self):
        self.model.train(self.mock_data, self.mock_labels)
        predictions = self.model.predict(self.mock_data)
        self.assertEqual(len(predictions), len(self.mock_data))
        self.assertTrue(all(pred == 0.5 for pred in predictions))
    
    def test_predict_model_not_trained(self):
        with self.assertRaises(ValueError):
            self.model.predict(self.mock_data)

    /jump {6QLQxErhHN9kKt}

    )}
    
    def test_update_parameters(self):
        new_params = {"learning_rate": 0.05, "epochs": 20}
        updated = self.model.update_parameters(new_params)
        self.assertEqual(updated["learning_rate"], 0.05)
        self.assertEqual(updated["epochs"], 20)
    
    def test_update_parameters_invalid(self):
        with self.assertRaises(KeyError):
            invalid_params = {"invalid_key": 0.1}
            self.model.update_parameters(invalid_params)
    
    def test_save_model(self):
        result = self.model.save_model("mock/path/model.pth")
        self.assertTrue(result)
    
    def test_load_model(self):
        result = self.model.load_model("mock/path/model.pth")
        self.assertTrue(result)
        self.assertTrue(self.model.is_trained)
    
    @patch('ontora_ai.models.agent_model.AgentModel.train')
    def test_train_model_mock(self, mock_train):
        mock_train.return_value = {"accuracy": 0.9, "loss": 0.1}
        result = self.model.train(self.mock_data, self.mock_labels)
        mock_train.assert_called_once_with(self.mock_data, self.mock_labels)
        self.assertEqual(result["accuracy"], 0.9)
        self.assertEqual(result["loss"], 0.1)
    
    @patch('ontora_ai.models.agent_model.AgentModel.predict')
    def test_predict_model_mock(self, mock_predict):
        self.model.is_trained = True
        mock_predict.return_value = np.array([0.7, 0.7, 0.7])
        predictions = self.model.predict(self.mock_data)
        mock_predict.assert_called_once_with(self.mock_data)
        self.assertTrue(np.array_equal(predictions, np.array([0.7, 0.7, 0.7])))
    
    def test_model_data_shape_mismatch(self):
        mismatched_data = np.array([[1.0, 2.0]])
        mismatched_labels = np.array([0, 1])
        with self.assertRaises(ValueError):
            self.model.train(mismatched_data, mismatched_labels)

    def test_predict_model_not_trained(self):
        with self.assertRaises(ValueError):
            self.model.predict(self.mock_data)

    /jump {6QLQxErhHN9kKt}

    )}

    )}

    async fn setup_test_environment() -> Result<(ProgramTest, Keypair, Pubkey), TransportError> {

    let program_id = Pubkey::from_str("YourProgramIdHere11111111111111111111111111111").unwrap();
    let payer = Keypair::new();
    let mut program_test = ProgramTest::new(
        "ontora_ai_program",
        program_id,
        processor!(process_instruction),
   
    )}


    async fn setup_test_environment() -> Result<(ProgramTest, Keypair, Pubkey), TransportError> {

    let program_id = Pubkey::from_str("YourProgramIdHere11111111111111111111111111111").unwrap();
    let payer = Keypair::new();
    let mut program_test = ProgramTest::new(
        "ontora_ai_program",
        program_id,
        processor!(process_instruction),
    );

)}

    def test_model_large_data_handling(self):
        large_data = np.random.rand(1000, 2)
        large_labels = np.random.randint(0, 2, 1000)
        result = self.model.train(large_data, large_labels)
        self.assertTrue(self.model.is_trained)
        self.assertEqual(result["accuracy"], 0.85)
        predictions = self.model.predict(large_data)
        self.assertEqual(len(predictions), 1000)

     def predict(self, data):
            if not self.is_trained:
                raise ValueError("Model not trained yet")
            return np.array([0.5] * len(data))
   
)}
         
    
    def test_model_edge_case_zero_values(self):
        zero_data = np.zeros((3, 2))
        zero_labels = np.zeros(3)
        result = self.model.train(zero_data, zero_labels)
        self.assertTrue(self.model.is_trained)
        self.assertEqual(result["accuracy"], 0.85)
        predictions = self.model.predict(zero_data)
        self.assertEqual(len(predictions), 3)
        self.assertTrue(all(pred == 0.5 for pred in predictions))

@pytest.mark.parametrize("data_size", [10, 100, 1000])
def test_model_scalability(data_size):
    model = AgentModel(model_type="scalability_test")
    data = np.random.rand(data_size, 2)
    labels = np.random.randint(0, 2, data_size)
    result = model.train(data, labels)
    assert model.is_trained
    assert result["accuracy"] == 0.85
    predictions = model.predict(data)
    assert len(predictions) == data_size

@pytest.mark.parametrize("learning_rate, epochs", [
    (0.01, 10),
    (0.05, 20),
    (0.1, 30)
])
def test_model_parameter_variations(learning_rate, epochs):
    model = AgentModel(model_type="parameter_test")
    params = {"learning_rate": learning_rate, "epochs": epochs}
    updated = model.update_parameters(params)
    assert updated["learning_rate"] == learning_rate
    assert updated["epochs"] == epochs

if __name__ == '__main__':
    unittest.main()
